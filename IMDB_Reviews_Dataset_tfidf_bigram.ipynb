{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Reviews Dataset\n",
    "\n",
    "The IMDB Review dataset contains movie reviews along with associated binary sentiment polarity labels.The main dataset contains 50,000 reviews split evenly between a train and test set (25,000 each). The distribution of positive and negative labels are balanced.\n",
    "\n",
    "In the entire dataset, no more than 30 movie reviews are allowed for the same movie. This is because reviews for the same movie tend to have correlation. Further, the train and test sets contain a disjoint set of movies, so no significant performance is obtained by memorizing movie-unique terms and their associated with observed labels.\n",
    "\n",
    "For the train/test set a negative review has a score <= 4 out of 10 and a positive review has a score >= 7 out of 10. Reviews with a more neutral score were not included in the dataset.\n",
    "\n",
    "Link: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "\n",
    "The objective is to perform a sentiment analysis on this dataset using various machine learning models\n",
    "\n",
    "\n",
    "## Loading the IMDB Dataset\n",
    "\n",
    "Format the dataset into lists of strings and review a few examples of positive and negative reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries (general)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset unpacked in aclImdb Folder\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import wget\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "#DownLoad IMDB Data to your working path from the link below\n",
    "URL = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "#Downloads the file to your current working directory\n",
    "#May need to install the wget package: conda install -c conda-forge python-wget\n",
    "if not os.path.exists('aclImdb_v1.tar.gz'):\n",
    "    wget.download(URL)\n",
    "\n",
    "#The file downloaded is in the aclImdb_v1.tar.gz file\n",
    "tar = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "print(\"Dataset unpacked in aclImdb Folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the original text files from the aclImdb folder and write the contents to a new text file.\n",
    "#The end result is 4 text files for positive and negative reviews in seperate train and test datasets\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "#Make a folder for the new text files\n",
    "if not os.path.exists('IMDB_Data'):\n",
    "    os.mkdir(\"IMDB_Data\")\n",
    "\n",
    "read_files = glob.glob(os.path.join('aclImdb/train/pos',\"*.txt\"))\n",
    "\n",
    "with open('IMDB_Data/pos_train.txt','wb') as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f,'rb') as infile:\n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "        outfile.write(b\"\\n\")\n",
    "        \n",
    "read_files = glob.glob(os.path.join('aclImdb/train/neg',\"*.txt\"))\n",
    "\n",
    "with open('IMDB_Data/neg_train.txt','wb') as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f,'rb') as infile:\n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "        outfile.write(b\"\\n\")\n",
    "        \n",
    "read_files = glob.glob(os.path.join('aclImdb/test/pos',\"*.txt\"))\n",
    "\n",
    "with open('IMDB_Data/pos_test.txt','wb') as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f,'rb') as infile:\n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "        outfile.write(b\"\\n\")\n",
    "        \n",
    "read_files = glob.glob(os.path.join('aclImdb/test/neg',\"*.txt\"))\n",
    "\n",
    "with open('IMDB_Data/neg_test.txt','wb') as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f,'rb') as infile:\n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "        outfile.write(b\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn the contents of the text files into a list of strings\n",
    "reviews_train_pos = []\n",
    "for line in open('IMDB_Data/pos_train.txt', 'r', encoding = \"utf8\"):\n",
    "    reviews_train_pos.append(line.strip())\n",
    "    \n",
    "reviews_train_neg = []\n",
    "for line in open('IMDB_Data/neg_train.txt', 'r', encoding = \"utf8\"):\n",
    "    reviews_train_neg.append(line.strip())\n",
    "    \n",
    "reviews_test_pos = []\n",
    "for line in open('IMDB_Data/pos_test.txt', 'r', encoding = \"utf8\"):\n",
    "    reviews_test_pos.append(line.strip())\n",
    "    \n",
    "reviews_test_neg = []\n",
    "for line in open('IMDB_Data/neg_test.txt', 'r', encoding = \"utf8\"):\n",
    "    reviews_test_neg.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n",
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews_train_pos))\n",
    "print(len(reviews_train_neg))\n",
    "print(len(reviews_test_pos))\n",
    "print(len(reviews_test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Positive Review Example\n",
    "reviews_train_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I\\'m a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another Positive Review Example\n",
    "reviews_train_pos[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Negative Review Example\n",
    "reviews_train_neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don't know.<br /><br />But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of his culinary skills and restaurant, and ultimately of himself and his youthful exploits, than of anybody or anything else. He never convinced me he was in love with the princess.<br /><br />I was disappointed in this movie. But, don't forget it was nominated for an Oscar, so judge for yourself.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another Negative Review Example\n",
    "reviews_train_neg[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Cleaning the Text Data\n",
    "\n",
    "As seen above, the original reviews are quite messy and need to be cleaned in order to help the machine learning models. This includes removing capital letters, removing punctuation, and any other uneccessary characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "replace_no_space = re.compile(\"[.;:!\\'?_,\\\"()\\[\\]]\")\n",
    "replace_with_space = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [replace_no_space.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [replace_with_space.sub(\" \", line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "reviews_train_pos_clean = preprocess_reviews(reviews_train_pos)\n",
    "reviews_train_neg_clean = preprocess_reviews(reviews_train_neg)\n",
    "reviews_test_pos_clean = preprocess_reviews(reviews_test_pos)\n",
    "reviews_test_neg_clean = preprocess_reviews(reviews_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The same postive review example after being cleaned\n",
    "reviews_train_pos_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'story of a man who has unnatural feelings for a pig starts out with a opening scene that is a terrific example of absurd comedy a formal orchestra audience is turned into an insane violent mob by the crazy chantings of its singers unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting even those from the era should be turned off the cryptic dialogue would make shakespeare seem easy to a third grader on a technical level its better than you might think with some good cinematography by future great vilmos zsigmond future stars sally kirkland and frederic forrest can be seen briefly'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The same negative review example after cleaning\n",
    "reviews_train_neg_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Further Text Processing: Removing Stop Words and Normalization\n",
    "\n",
    "Other methods of cleaning the data that can change the model performance include removing stop_words or Normalization (Stemming or Lematization)\n",
    "\n",
    "Stop words are very common words such as 'in', 'of', 'a', 'at', or 'the' that usually don't provide any useful information to the text classifier\n",
    "\n",
    "Normalization (Stemming or Lematization) is a common next step in text preprocessing that converts all the different forms of a certain word into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\patri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Stemming\n",
    "def stemming_text(text_data):\n",
    "    stem = PorterStemmer()\n",
    "    return [' '.join([stem.stem(word) for word in review.split()]) for review in text_data]\n",
    "\n",
    "\n",
    "#Lemmatization\n",
    "def lemmatize_text(text_data):\n",
    "    lem = WordNetLemmatizer()\n",
    "    return [' '.join([lem.lemmatize(word) for word in review.split()]) for review in text_data]\n",
    "\n",
    "    \n",
    "#Alternatively there is an easier way to remove stop words by using the stop_words argument with any of scikit-learn’s ‘Vectorizer’ classes\n",
    "#Removing stop words often (but not always) improves the model accuracy   \n",
    "#Need to create the list of stop_words (usually more effective than general lists)\n",
    "#stop_words=['in','of','at','a','the']\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data Sets for Feature Vectorization and Classification Models\n",
    "\n",
    "With these datasets the positve and negative results will be combined and the data needs to be shuffled since the positive and negative data is grouped together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "#Combine all the positive and negative reviews to end up with one training set and one testing set of 25,000 samples each\n",
    "reviews_train = []\n",
    "reviews_train_combined = reviews_train_pos_clean + reviews_train_neg_clean\n",
    "print(len(reviews_train_combined))\n",
    "\n",
    "reviews_test = []\n",
    "reviews_test_combined = reviews_test_pos_clean + reviews_test_neg_clean\n",
    "print(len(reviews_test_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you might be a bit confused if you watch this silly made for from the beginning since the credits list susan dey as special guest star um why would a one off mow like this have a guest star well if you stick with it youll find yourself paying attention to little else but ms deys butt wiggling in a flowered bikini as the partridge family house babe frolics on the beach to which that imaginative title alludes susans derrière is especially compelling when she shakes it at the camera while teasing and tickling her pseudo disaffected brother in one mildly incestuous scene sadly susie and her tush fight a losing battle the jiggle tv craze that might have put that bottom over the top was three years off so that sweet booty just gets a supporting role in 1976 fat freddy silverman would have put that behind right out front and used this flick as susans audition tape for charlies angels as is our susan was denied cheesecake immortality and had to settle for a very commendable career playing somber neurotic women the view beyond susans heinie it must be said is not very compelling the scenery is nice and photographed in a bizarre hazy way that briefly fools you into thinking there might be some quirky creative intelligence at work behind the camera nope its just a typically suspense challenged 70s made for tv thriller that allowed weekly series stars to make some extra cashand collect some cable residuals though they obviously didnt know that at the time and show off their range here were treated to a tv scale nuclear family squaring off against tv scale thugs who cant decide whether theyre a motorcycle gang or a hippie cult thus the filmmakers split the difference by putting them in dune buggies and have never learned one of the primary lessons of 1970s television dont mess with dennis weaver see mccloud and duel the only potential for depth in this movie is in the aforementioned teenage son character of steve played by the long forgotten if ever remembered kristoffer tabori who is supposed to be rebellious and troubled and might feel some sympathy for and attraction to the lawless mob that is supposedly menacing his family but steve as played by tabori gosh why didnt we see more from this wunderkind is actually just grumpy and moody and isnt one bit conflicted when big d gets serious and draws a line in the proverbial and literal sand for the sleep deprived and susan deyniacs there must be some of you out there only\n",
      "0\n",
      "an unflinching descent into psychological and physical oblivion that will undoubtedly burn images of the truthful brutality and suffering of war into your cerebral cortex in a way not many other films will in fact there is simply no other war film like it director kon ichikawa witnessed the unthinkable horror of hiroshima first hand only 10 days after the bomb was dropped he has said that from that day it would always be his mission to express the pointless empty violence humans inflict on each other and themselves  mr ichikawa shows us that there are no winners in war for the paths to victory and defeat are paved with the same soldiers soullessly marching down roads which only have death and destruction at their end mr ichikawa succeeds in bringing his message to the world thru this haunting piece of cinema\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Shuffle\n",
    "import random\n",
    "\n",
    "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "c = list(zip(reviews_train_combined, target))\n",
    "random.shuffle(c)\n",
    "reviews_train, y_train = zip(*c)\n",
    "\n",
    "print(reviews_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "b = list(zip(reviews_test_combined, target))\n",
    "random.shuffle(b)\n",
    "reviews_test, y_test = zip(*b)\n",
    "\n",
    "print(reviews_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vectorization (Count Vectorizer and TF-IDF)\n",
    "\n",
    "### Can use either the Unigram Vectorization Features, Bigram Vectorization Features, or TF-IDF Features in the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 92446)\n",
      "  (0, 52880)\t4\n",
      "  (0, 9465)\t2\n",
      "  (0, 17606)\t1\n",
      "  (0, 88939)\t1\n",
      "  (0, 74467)\t1\n",
      "  (0, 49804)\t2\n",
      "  (0, 8393)\t1\n",
      "  (0, 74600)\t1\n",
      "  (0, 19162)\t1\n",
      "  (0, 48266)\t1\n",
      "  (0, 79772)\t3\n",
      "  (0, 22354)\t1\n",
      "  (0, 76625)\t1\n",
      "  (0, 35842)\t2\n",
      "  (0, 77524)\t2\n",
      "  (0, 85194)\t1\n",
      "  (0, 90956)\t2\n",
      "  (0, 58421)\t4\n",
      "  (0, 54914)\t1\n",
      "  (0, 47949)\t1\n",
      "  (0, 89382)\t1\n",
      "  (0, 77958)\t1\n",
      "  (0, 91831)\t1\n",
      "  (0, 30488)\t1\n",
      "  (0, 60816)\t1\n",
      "  :\t:\n",
      "  (24999, 56379)\t1\n",
      "  (24999, 50327)\t1\n",
      "  (24999, 17526)\t1\n",
      "  (24999, 28374)\t1\n",
      "  (24999, 43065)\t1\n",
      "  (24999, 24273)\t1\n",
      "  (24999, 79175)\t1\n",
      "  (24999, 60512)\t1\n",
      "  (24999, 25247)\t1\n",
      "  (24999, 6473)\t1\n",
      "  (24999, 19698)\t1\n",
      "  (24999, 36052)\t1\n",
      "  (24999, 60706)\t1\n",
      "  (24999, 31684)\t1\n",
      "  (24999, 35964)\t1\n",
      "  (24999, 30625)\t1\n",
      "  (24999, 26006)\t1\n",
      "  (24999, 10051)\t1\n",
      "  (24999, 20720)\t1\n",
      "  (24999, 92150)\t1\n",
      "  (24999, 74061)\t1\n",
      "  (24999, 53050)\t1\n",
      "  (24999, 8515)\t1\n",
      "  (24999, 31637)\t1\n",
      "  (24999, 33515)\t1\n"
     ]
    }
   ],
   "source": [
    "#Feature Vectorization (Unigram)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#stop_words parameter: If ‘english’, a built-in stop word list for English is used. There are several known issues with ‘english’ and you should consider an alternative (see Using stop words).\n",
    "#ngram_range parameter: The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
    "#max_features parameter: Max number of features\n",
    "vectorizer = CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)\n",
    "\n",
    "X_train_counts = vectorizer.fit_transform(reviews_train)\n",
    "X_test_counts = vectorizer.transform(reviews_test)\n",
    "\n",
    "print(X_train_counts.shape)\n",
    "print(X_train_counts)\n",
    "#print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1913493)\n",
      "  (0, 1072112)\t4\n",
      "  (0, 183691)\t2\n",
      "  (0, 341633)\t1\n",
      "  (0, 1828345)\t1\n",
      "  (0, 1525495)\t1\n",
      "  (0, 1013269)\t2\n",
      "  (0, 163354)\t1\n",
      "  (0, 1530042)\t1\n",
      "  (0, 377813)\t1\n",
      "  (0, 977303)\t1\n",
      "  (0, 1646035)\t3\n",
      "  (0, 436495)\t1\n",
      "  (0, 1572083)\t1\n",
      "  (0, 745007)\t2\n",
      "  (0, 1587163)\t2\n",
      "  (0, 1762700)\t1\n",
      "  (0, 1888429)\t2\n",
      "  (0, 1185182)\t4\n",
      "  (0, 1115469)\t1\n",
      "  (0, 964119)\t1\n",
      "  (0, 1844225)\t1\n",
      "  (0, 1599214)\t1\n",
      "  (0, 1906653)\t1\n",
      "  (0, 630714)\t1\n",
      "  (0, 1231663)\t1\n",
      "  :\t:\n",
      "  (24999, 1056204)\t1\n",
      "  (24999, 513636)\t1\n",
      "  (24999, 194211)\t1\n",
      "  (24999, 988782)\t1\n",
      "  (24999, 1910177)\t1\n",
      "  (24999, 127726)\t1\n",
      "  (24999, 88896)\t1\n",
      "  (24999, 1240817)\t1\n",
      "  (24999, 216539)\t1\n",
      "  (24999, 489472)\t1\n",
      "  (24999, 694502)\t1\n",
      "  (24999, 1111344)\t1\n",
      "  (24999, 1256860)\t1\n",
      "  (24999, 1508476)\t1\n",
      "  (24999, 1518833)\t1\n",
      "  (24999, 694471)\t1\n",
      "  (24999, 394960)\t1\n",
      "  (24999, 691703)\t1\n",
      "  (24999, 885869)\t1\n",
      "  (24999, 921574)\t1\n",
      "  (24999, 1447580)\t1\n",
      "  (24999, 636263)\t1\n",
      "  (24999, 1383668)\t1\n",
      "  (24999, 1459390)\t1\n",
      "  (24999, 1395088)\t1\n"
     ]
    }
   ],
   "source": [
    "#Feature Vectorization (Bigram)\n",
    "vectorizer_bigram = CountVectorizer(stop_words = stop_words, ngram_range=(1,2), max_features = None)\n",
    "\n",
    "X_train_counts_bigram = vectorizer_bigram.fit_transform(reviews_train)\n",
    "X_test_counts_bigram = vectorizer_bigram.transform(reviews_test)\n",
    "\n",
    "print(X_train_counts_bigram.shape)\n",
    "print(X_train_counts_bigram)\n",
    "#print(vectorizer_bigram.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1913493)\n",
      "  (0, 1906739)\t0.0362918911998756\n",
      "  (0, 1906653)\t0.02189625994499474\n",
      "  (0, 1902273)\t0.053552170744055556\n",
      "  (0, 1900961)\t0.015569234360424\n",
      "  (0, 1897991)\t0.05571744890028493\n",
      "  (0, 1897990)\t0.052015881443294144\n",
      "  (0, 1890259)\t0.037922700366592045\n",
      "  (0, 1890059)\t0.03773530454204102\n",
      "  (0, 1888429)\t0.022797313121946994\n",
      "  (0, 1878946)\t0.050824242883768043\n",
      "  (0, 1878764)\t0.015844526708482975\n",
      "  (0, 1873334)\t0.052015881443294144\n",
      "  (0, 1872314)\t0.021334084527807013\n",
      "  (0, 1860087)\t0.05571744890028493\n",
      "  (0, 1860084)\t0.04902740273756439\n",
      "  (0, 1853126)\t0.04831431398630336\n",
      "  (0, 1852641)\t0.023843159739231394\n",
      "  (0, 1846759)\t0.050824242883768043\n",
      "  (0, 1844225)\t0.01180924794363647\n",
      "  (0, 1842647)\t0.053552170744055556\n",
      "  (0, 1842614)\t0.03921056194406138\n",
      "  (0, 1841312)\t0.05571744890028493\n",
      "  (0, 1841286)\t0.041436871999031795\n",
      "  (0, 1835512)\t0.053552170744055556\n",
      "  (0, 1835183)\t0.012957132154609417\n",
      "  :\t:\n",
      "  (24999, 216079)\t0.029892037848167227\n",
      "  (24999, 213666)\t0.07423795688620533\n",
      "  (24999, 213632)\t0.043014062631202006\n",
      "  (24999, 194211)\t0.07423795688620533\n",
      "  (24999, 194210)\t0.0713529391838249\n",
      "  (24999, 177171)\t0.05473597797287409\n",
      "  (24999, 175463)\t0.019154397285085067\n",
      "  (24999, 174231)\t0.0713529391838249\n",
      "  (24999, 172999)\t0.018725579076518097\n",
      "  (24999, 166598)\t0.0713529391838249\n",
      "  (24999, 166597)\t0.06771824673329409\n",
      "  (24999, 149142)\t0.06642096884585674\n",
      "  (24999, 149112)\t0.046055467489208454\n",
      "  (24999, 137191)\t0.07423795688620533\n",
      "  (24999, 136754)\t0.020296968070443462\n",
      "  (24999, 127726)\t0.07423795688620533\n",
      "  (24999, 127719)\t0.06353595114347631\n",
      "  (24999, 122719)\t0.0713529391838249\n",
      "  (24999, 122658)\t0.0482623218910881\n",
      "  (24999, 92045)\t0.06930598654823716\n",
      "  (24999, 91827)\t0.0241578166278738\n",
      "  (24999, 88896)\t0.07423795688620533\n",
      "  (24999, 87379)\t0.020908662868934304\n",
      "  (24999, 68001)\t0.07423795688620533\n",
      "  (24999, 66668)\t0.01678749458527389\n"
     ]
    }
   ],
   "source": [
    "#Feature Vectorization (TF-IDF)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts_bigram)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts_bigram)\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Classifier Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = reviews_train\n",
    "X_test = reviews_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train_counts, target, train_size=0.8, test_size = 0.2)\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_val.shape)\n",
    "#print(X_val.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Pipeline for Cross Validation\n",
    "\n",
    "A pipeline for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.8852  0.88516 0.88516 0.88516 0.89812 0.89828 0.89852 0.89864 0.8852\n",
      " 0.88516 0.88516 0.88516 0.89812 0.89828 0.89852 0.89864]\n",
      "scores_std [0.00341292 0.00334521 0.00334521 0.00334521 0.00194566 0.00207307\n",
      " 0.00256    0.00288    0.00341292 0.00334521 0.00334521 0.00334521\n",
      " 0.00194566 0.00207307 0.00256    0.00288   ]\n",
      "Best score: 0.899\n",
      "\n",
      " Best Parameter Values: \n",
      "clf__max_iter: 5000\n",
      "clf__penalty: 'none'\n",
      "clf__tol: 0.0001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "#Tol parameter:Tolerance for stopping criteria\n",
    "#penalty parameter:Used to specify the norm used in the penalization\n",
    "#max_iter parameter: Maximum number of iterations\n",
    "parameters = {'clf__tol': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "             'clf__penalty': ['l2','none'],\n",
    "             'clf__max_iter': [5000 , 10000]}\n",
    "n_folds = 5\n",
    "\n",
    "LR_GridSearch = GridSearchCV(pipeline, param_grid = parameters, cv=n_folds)\n",
    "LR_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = LR_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = LR_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % LR_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, LR_GridSearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [1 0 1 ... 0 1 0]\n",
      "Accuracy:  89.11200000000001 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pipeline = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(penalty = 'none', max_iter = 5000, tol = 0.0001))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_predLR = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Predicted: \", y_predLR)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_predLR)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Which Features (Words) Were the Most Valuable for Classification as Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most likely indicating a positive review: \n",
      "('excellent', 1.3801997925826957)\n",
      "('perfect', 1.196436589741343)\n",
      "('wonderful', 1.0995069134783602)\n",
      "('superb', 1.0514300818372568)\n",
      "('amazing', 1.011476751715405)\n",
      "('funniest', 0.9820910494554003)\n",
      "('favorite', 0.9734102548564036)\n",
      "('must see', 0.9624084833390034)\n",
      "('enjoyable', 0.9331565309659303)\n",
      "('10 10', 0.9308334564569019)\n",
      "('wonderfully', 0.8980885752513412)\n",
      "('rare', 0.8952396063051214)\n",
      "('enjoyed', 0.8806181946286363)\n",
      "('brilliant', 0.8699764758021181)\n",
      "('well worth', 0.8632120665495107)\n",
      "('refreshing', 0.8556848472132872)\n",
      "('fantastic', 0.8425594701791768)\n",
      "('today', 0.8105063390870713)\n",
      "('hilarious', 0.8047820258537225)\n",
      "('touching', 0.8019280254967668)\n",
      "\n",
      " Words most likely indicating a negative review: \n",
      "('worst', -2.0704093392208924)\n",
      "('waste', -1.660321572656556)\n",
      "('awful', -1.6326175861195817)\n",
      "('disappointment', -1.4644799009424696)\n",
      "('boring', -1.4421694536049476)\n",
      "('poorly', -1.315077825519572)\n",
      "('disappointing', -1.2209432037433257)\n",
      "('dull', -1.2116312539137508)\n",
      "('worse', -1.184746691971555)\n",
      "('horrible', -1.1832167568130052)\n",
      "('terrible', -1.1777578591633504)\n",
      "('poor', -1.1482696935589296)\n",
      "('weak', -1.1010847648757949)\n",
      "('fails', -1.0838679180643949)\n",
      "('lacks', -1.0788837420438793)\n",
      "('unfortunately', -1.0745386748973935)\n",
      "('avoid', -1.0561512367073758)\n",
      "('save', -1.0487486447597933)\n",
      "('mess', -1.0446717711200393)\n",
      "('lame', -1.0042739126399667)\n"
     ]
    }
   ],
   "source": [
    "clf_LR = LogisticRegression(penalty = 'l2', max_iter = 5000, tol = 0.1)\n",
    "clf_LR.fit(X_train_counts_bigram, y_train)\n",
    "pred = clf_LR.predict(X_test_counts_bigram)\n",
    "\n",
    "feature_to_coef = {word: coef for word, coef in zip(vectorizer_bigram.get_feature_names(), clf_LR.coef_[0])}\n",
    "\n",
    "print(\"Words most likely indicating a positive review: \")\n",
    "for most_positive in sorted(feature_to_coef.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print (most_positive)\n",
    "  \n",
    "print(\"\\n Words most likely indicating a negative review: \")\n",
    "for most_negative in sorted(feature_to_coef.items(), key=lambda x: x[1])[:20]:\n",
    "    print (most_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.88332 0.8852  0.888   0.88804]\n",
      "scores_std [0.00262176 0.00400999 0.00418951 0.00379136]\n",
      "Best score: 0.888\n",
      "\n",
      " Best Parameter Values: \n",
      "clf__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipelineMNB = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "#alpha parameter: Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing)\n",
    "parameters = {'clf__alpha': [1, 0.5, 0.2, 0.1]}\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "MNB_GridSearch = GridSearchCV(pipelineMNB, param_grid = parameters, cv=n_folds)\n",
    "MNB_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = MNB_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = MNB_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % MNB_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, MNB_GridSearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [1 1 1 ... 1 0 1]\n",
      "Accuracy:  85.776 %\n"
     ]
    }
   ],
   "source": [
    "pipelineMNB = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB(alpha = 0.1))\n",
    "])\n",
    "\n",
    "pipelineMNB.fit(X_train, y_train)\n",
    "y_predMNB = pipelineMNB.predict(X_test)\n",
    "\n",
    "print(\"Predicted: \", y_predMNB)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_predMNB)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.5288  0.5     0.50472 0.61984 0.50348 0.5666  0.56856 0.5828  0.588\n",
      " 0.81716 0.82236 0.79952 0.8066  0.8108  0.81112 0.84424 0.84396 0.84416\n",
      " 0.8948  0.89492 0.8946  0.87564 0.87624 0.87692]\n",
      "scores_std [0.05471716 0.         0.00934032 0.1207363  0.00505585 0.09230003\n",
      " 0.10505826 0.09475636 0.0651552  0.01638507 0.01257515 0.04111479\n",
      " 0.00462861 0.00520154 0.00267313 0.00461198 0.00474704 0.00429027\n",
      " 0.00221991 0.0024285  0.00235627 0.00331156 0.00328731 0.0042078 ]\n",
      "Best score: 0.895\n",
      "\n",
      " Best Parameter Values: \n",
      "clf__alpha: 0.0001\n",
      "clf__loss: 'hinge'\n",
      "clf__max_iter: 80\n",
      "clf__penalty: 'l2'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "pipelineSGD = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier())\n",
    "])\n",
    "\n",
    "#loss parameter:The loss function to be used\n",
    "#penalty parameter:The penalty (aka regularization term) to be used\n",
    "#alpha parameter:Constant that multiplies the regularization term\n",
    "#max_iter parameter:The maximum number of passes over the training data (aka epochs)\n",
    "parameters = {'clf__loss': ['hinge', 'log'],\n",
    "             'clf__penalty': ['l2'],\n",
    "             'clf__alpha': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "             'clf__max_iter': [60, 80, 100]}\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "SGD_GridSearch = GridSearchCV(pipelineSGD, param_grid = parameters, cv=n_folds)\n",
    "SGD_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = SGD_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = SGD_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % SGD_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, SGD_GridSearch.best_params_[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [1 1 1 ... 1 0 1]\n",
      "Accuracy:  88.732 %\n"
     ]
    }
   ],
   "source": [
    "pipelineSGD = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, max_iter=80))\n",
    "])\n",
    "\n",
    "pipelineSGD.fit(X_train, y_train)\n",
    "\n",
    "y_predSGD = pipelineSGD.predict(X_test)\n",
    "print(\"Predicted: \", y_predSGD)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_predSGD)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.63    0.58956 0.70236 0.52756 0.50428 0.69208 0.53548 0.50208 0.72364\n",
      " 0.54956 0.50232 0.72584 0.58428 0.50564 0.71476 0.64788 0.58404 0.70488\n",
      " 0.52912 0.50132 0.69116 0.53008 0.5004  0.71812 0.57724 0.50228 0.7216\n",
      " 0.58524 0.50596 0.71312]\n",
      "scores_std [0.00711393 0.01550608 0.00803134 0.01210976 0.00737629 0.00507007\n",
      " 0.01256127 0.00264    0.00617142 0.02177279 0.00060133 0.0039948\n",
      " 0.01049617 0.00272    0.00550113 0.00831923 0.00818916 0.00204196\n",
      " 0.0220989  0.00051536 0.00484215 0.01167466 0.00110272 0.00694734\n",
      " 0.01766687 0.00121062 0.00355753 0.01254649 0.00367565 0.00286384]\n",
      "Best score: 0.726\n",
      "\n",
      " Best Parameter Values: \n",
      "clf__criterion: 'gini'\n",
      "clf__max_depth: 20\n",
      "clf__max_features: None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipeline_tree = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "#criterion parameter:The function to measure the quality of a split.\n",
    "#max_depth parameter:The maximum depth of the tree. \n",
    "#max_features parameter:The number of features to consider when looking for the best split. If “sqrt”, then max_features=sqrt(n_features). If “log2”, then max_features=log2(n_features).\n",
    "parameters = {'clf__criterion': ['gini', 'entropy'],\n",
    "             'clf__max_depth': [None, 5, 10, 20, 40],\n",
    "             'clf__max_features': ['sqrt', 'log2', None]}\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "tree_GridSearch = GridSearchCV(pipeline_tree, param_grid = parameters, cv=n_folds)\n",
    "tree_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = tree_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = tree_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % tree_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, tree_GridSearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [1 0 1 ... 0 1 1]\n",
      "Accuracy:  73.072 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipeline_tree = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', DecisionTreeClassifier(criterion ='gini', max_depth = 20, max_features = None))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = pipeline_tree.predict(X_test)\n",
    "print(\"Predicted: \", y_pred_tree)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred_tree)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.90112 0.90104 0.90104 0.90104 0.90076 0.90108 0.90104 0.90104 0.90088\n",
      " 0.90104 0.90104 0.90104 0.89844 0.89844 0.89844 0.89844 0.89852 0.89844\n",
      " 0.89844 0.89844 0.8984  0.89844 0.89844 0.89844]\n",
      "scores_std [0.00272353 0.0029104  0.0029104  0.0029104  0.00272881 0.00287221\n",
      " 0.0029104  0.0029104  0.00281027 0.0029104  0.0029104  0.0029104\n",
      " 0.00232345 0.0024377  0.0024377  0.0024377  0.00252539 0.0024377\n",
      " 0.0024377  0.0024377  0.00236981 0.0024377  0.0024377  0.0024377 ]\n",
      "Best score: 0.901\n",
      "\n",
      " Best Parameter Values: \n",
      "clf__C: 1.0\n",
      "clf__max_iter: 1000\n",
      "clf__penalty: 'l2'\n",
      "clf__tol: 0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pipelineSVM = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "#penalty parameter:Specifies the norm used in the penalization\n",
    "#tol parameter:Tolerance for stopping criteria.\n",
    "#C parameter: Regularization parameter. The strength of the regularization is inversely proportional to C.\n",
    "#max_iter parameter:The maximum number of iterations to be run\n",
    "parameters = {'clf__penalty': ['l2'],\n",
    "             'clf__tol': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'clf__C': [1.0, 0.5],\n",
    "             'clf__max_iter': [1000, 1500, 2000]}\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "SVM_GridSearch = GridSearchCV(pipelineSVM, param_grid = parameters, cv=n_folds)\n",
    "SVM_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = SVM_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = SVM_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % SVM_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, SVM_GridSearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [1 0 1 ... 0 1 0]\n",
      "Accuracy:  89.4 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "pipelineSVM = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC(C = 1, max_iter = 1000, penalty = 'l2', tol = 0.1))\n",
    "])\n",
    "\n",
    "\n",
    "pipelineSVM.fit(X_train, y_train)\n",
    "\n",
    "y_predSVM = pipelineSVM.predict(X_test)\n",
    "print(\"Predicted: \", y_predSVM)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_predSVM)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.72668 0.76444 0.78256 0.79276 0.79524 0.82396 0.83564 0.84388 0.80144\n",
      " 0.8248  0.83752 0.84232]\n",
      "scores_std [0.00598478 0.00510122 0.00462238 0.00433294 0.00477728 0.00479233\n",
      " 0.00484545 0.0066388  0.00233032 0.00554112 0.00610783 0.00409995]\n",
      "Best score: 0.844\n",
      "\n",
      " Best Parameter Values: \n",
      "clf__base_estimator: None\n",
      "clf__learning_rate: 0.5\n",
      "clf__n_estimators: 200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "pipelineADA = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "#base_estimator parameter: The base estimator from which the boosted ensemble is built.\n",
    "#n_estimators: The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n",
    "#learning_rate:Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators.\n",
    "parameters = {'clf__base_estimator': [None],\n",
    "              'clf__n_estimators': [50, 100, 150, 200],\n",
    "             'clf__learning_rate': [0.1, 0.5, 1]}\n",
    "n_folds = 5\n",
    "\n",
    "Ada_GridSearch = GridSearchCV(pipelineADA, param_grid = parameters, cv=n_folds)\n",
    "Ada_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = Ada_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = Ada_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % Ada_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, Ada_GridSearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [0 1 1 ... 0 1 0]\n",
      "Accuracy:  86.61999999999999 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "pipelineADA = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', AdaBoostClassifier(base_estimator = None, n_estimators = 500, learning_rate = 0.5))\n",
    "])\n",
    "\n",
    "pipelineADA.fit(X_train, y_train)\n",
    "\n",
    "y_pred_Ada = pipelineADA.predict(X_test)\n",
    "print(\"Predicted: \", y_pred_Ada)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred_Ada)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipelineForest = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "#n_estimators parameter:The number of trees in the forest\n",
    "#criterion parameter:The function to measure the quality of a split (gini or entropy)\n",
    "#max_depth parameter:The maximum depth of the tree.\n",
    "#max_features parameter:The number of features to consider when looking for the best split. If “sqrt”, then max_features=sqrt(n_features) (same as “auto”). If “log2”, then max_features=log2(n_features). If None, then max_features=n_features.\n",
    "#bootstrap parameter: Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree.\n",
    "parameters = {'clf__n_estimators': [50, 70, 90],\n",
    "             'clf__criterion': ['gini', 'entropy'],\n",
    "             'clf__max_depth': [None, 10, 20],\n",
    "             'clf__max_features': ['sqrt', 'log2', None],\n",
    "             'clf__bootstrap': [True, False]}\n",
    "n_folds = 5\n",
    "\n",
    "forest_GridSearch = GridSearchCV(pipelineForest, param_grid = parameters, cv=n_folds)\n",
    "forest_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = forest_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = forest_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % forest_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, forest_GridSearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [1 1 1 ... 0 1 0]\n",
      "Accuracy:  86.372 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipelineForest = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier(n_estimators = 120, criterion = 'entropy', max_depth = None, max_features = 'sqrt', bootstrap = False))\n",
    "])\n",
    "\n",
    "pipelineForest.fit(X_train, y_train)\n",
    "\n",
    "y_pred_forest = pipelineForest.predict(X_test)\n",
    "print(\"Predicted: \", y_pred_forest)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred_forest)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze which Model is the Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:  89.11200000000001 %\n",
      "Logistic Regression Recall:  88.544 %\n",
      "Logistic Regression Precision:  89.56141770513028 % \n",
      "\n",
      "Decision Tree Accuracy:  73.072 %\n",
      "Decision Tree Recall:  80.816 %\n",
      "Decision Tree Precision:  69.9778331947908 % \n",
      "\n",
      "SVM Accuracy:  89.4 %\n",
      "SVM Recall:  89.288 %\n",
      "SVM Precision:  89.48845413726748 % \n",
      "\n",
      "Adaboost Accuracy:  86.61999999999999 %\n",
      "Adaboost Recall:  87.776 %\n",
      "Adaboost Precision:  85.7924779107045 % \n",
      "\n",
      "Random Forest Accuracy:  86.372 %\n",
      "Random Forest Recall:  85.024 %\n",
      "Random Forest Precision:  87.37975828331827 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Accuracy: \", metrics.accuracy_score(y_test, y_predLR)*100, \"%\")\n",
    "print(\"Logistic Regression Recall: \", metrics.recall_score(y_test, y_predLR)*100, \"%\")\n",
    "print(\"Logistic Regression Precision: \", metrics.precision_score(y_test, y_predLR)*100, \"%\", '\\n')\n",
    "\n",
    "print(\"Decision Tree Accuracy: \", metrics.accuracy_score(y_test, y_pred_tree)*100, \"%\")\n",
    "print(\"Decision Tree Recall: \", metrics.recall_score(y_test, y_pred_tree)*100, \"%\")\n",
    "print(\"Decision Tree Precision: \", metrics.precision_score(y_test, y_pred_tree)*100, \"%\", '\\n')\n",
    "\n",
    "print(\"SVM Accuracy: \", metrics.accuracy_score(y_test, y_predSVM)*100, \"%\")\n",
    "print(\"SVM Recall: \", metrics.recall_score(y_test, y_predSVM)*100, \"%\")\n",
    "print(\"SVM Precision: \", metrics.precision_score(y_test, y_predSVM)*100, \"%\", '\\n')\n",
    "\n",
    "print(\"Adaboost Accuracy: \", metrics.accuracy_score(y_test, y_pred_Ada)*100, \"%\")\n",
    "print(\"Adaboost Recall: \", metrics.recall_score(y_test, y_pred_Ada)*100, \"%\")\n",
    "print(\"Adaboost Precision: \", metrics.precision_score(y_test, y_pred_Ada)*100, \"%\", '\\n')\n",
    "\n",
    "print(\"Random Forest Accuracy: \", metrics.accuracy_score(y_test, y_pred_forest)*100, \"%\")\n",
    "print(\"Random Forest Recall: \", metrics.recall_score(y_test, y_pred_forest)*100, \"%\")\n",
    "print(\"Random Forest Precision: \", metrics.precision_score(y_test, y_pred_forest)*100, \"%\", '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}