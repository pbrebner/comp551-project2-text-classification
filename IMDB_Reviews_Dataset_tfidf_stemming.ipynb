{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Reviews Dataset\n",
    "\n",
    "The IMDB Review dataset contains movie reviews along with associated binary sentiment polarity labels.The main dataset contains 50,000 reviews split evenly between a train and test set (25,000 each). The distribution of positive and negative labels are balanced.\n",
    "\n",
    "In the entire dataset, no more than 30 movie reviews are allowed for the same movie. This is because reviews for the same movie tend to have correlation. Further, the train and test sets contain a disjoint set of movies, so no significant performance is obtained by memorizing movie-unique terms and their associated with observed labels.\n",
    "\n",
    "For the train/test set a negative review has a score <= 4 out of 10 and a positive review has a score >= 7 out of 10. Reviews with a more neutral score were not included in the dataset.\n",
    "\n",
    "Link: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "\n",
    "The objective is to perform a sentiment analysis on this dataset using various machine learning models\n",
    "\n",
    "\n",
    "## Loading the IMDB Dataset\n",
    "\n",
    "Format the dataset into lists of strings and review a few examples of positive and negative reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries (general)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset unpacked in aclImdb Folder\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import wget\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "#DownLoad IMDB Data to your working path from the link below\n",
    "URL = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "#Downloads the file to your current working directory\n",
    "#May need to install the wget package: conda install -c conda-forge python-wget\n",
    "if not os.path.exists('aclImdb_v1.tar.gz'):\n",
    "    wget.download(URL)\n",
    "\n",
    "#The file downloaded is in the aclImdb_v1.tar.gz file\n",
    "tar = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "print(\"Dataset unpacked in aclImdb Folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the original text files from the aclImdb folder and write the contents to a new text file.\n",
    "#The end result is 4 text files for positive and negative reviews in seperate train and test datasets\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "#Make a folder for the new text files\n",
    "if not os.path.exists('IMDB_Data'):\n",
    "    os.mkdir(\"IMDB_Data\")\n",
    "\n",
    "read_files = glob.glob(os.path.join('aclImdb/train/pos',\"*.txt\"))\n",
    "\n",
    "with open('IMDB_Data/pos_train.txt','wb') as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f,'rb') as infile:\n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "        outfile.write(b\"\\n\")\n",
    "        \n",
    "read_files = glob.glob(os.path.join('aclImdb/train/neg',\"*.txt\"))\n",
    "\n",
    "with open('IMDB_Data/neg_train.txt','wb') as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f,'rb') as infile:\n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "        outfile.write(b\"\\n\")\n",
    "        \n",
    "read_files = glob.glob(os.path.join('aclImdb/test/pos',\"*.txt\"))\n",
    "\n",
    "with open('IMDB_Data/pos_test.txt','wb') as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f,'rb') as infile:\n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "        outfile.write(b\"\\n\")\n",
    "        \n",
    "read_files = glob.glob(os.path.join('aclImdb/test/neg',\"*.txt\"))\n",
    "\n",
    "with open('IMDB_Data/neg_test.txt','wb') as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f,'rb') as infile:\n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "        outfile.write(b\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn the contents of the text files into a list of strings\n",
    "reviews_train_pos = []\n",
    "for line in open('IMDB_Data/pos_train.txt', 'r', encoding = \"utf8\"):\n",
    "    reviews_train_pos.append(line.strip())\n",
    "    \n",
    "reviews_train_neg = []\n",
    "for line in open('IMDB_Data/neg_train.txt', 'r', encoding = \"utf8\"):\n",
    "    reviews_train_neg.append(line.strip())\n",
    "    \n",
    "reviews_test_pos = []\n",
    "for line in open('IMDB_Data/pos_test.txt', 'r', encoding = \"utf8\"):\n",
    "    reviews_test_pos.append(line.strip())\n",
    "    \n",
    "reviews_test_neg = []\n",
    "for line in open('IMDB_Data/neg_test.txt', 'r', encoding = \"utf8\"):\n",
    "    reviews_test_neg.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n",
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews_train_pos))\n",
    "print(len(reviews_train_neg))\n",
    "print(len(reviews_test_pos))\n",
    "print(len(reviews_test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Positive Review Example\n",
    "reviews_train_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I\\'m a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another Positive Review Example\n",
    "reviews_train_pos[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Negative Review Example\n",
    "reviews_train_neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don't know.<br /><br />But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of his culinary skills and restaurant, and ultimately of himself and his youthful exploits, than of anybody or anything else. He never convinced me he was in love with the princess.<br /><br />I was disappointed in this movie. But, don't forget it was nominated for an Oscar, so judge for yourself.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another Negative Review Example\n",
    "reviews_train_neg[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Cleaning the Text Data\n",
    "\n",
    "As seen above, the original reviews are quite messy and need to be cleaned in order to help the machine learning models. This includes removing capital letters, removing punctuation, and any other uneccessary characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "replace_no_space = re.compile(\"[.;:!\\'?_,\\\"()\\[\\]]\")\n",
    "replace_with_space = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [replace_no_space.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [replace_with_space.sub(\" \", line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "reviews_train_pos_clean1 = preprocess_reviews(reviews_train_pos)\n",
    "reviews_train_neg_clean1 = preprocess_reviews(reviews_train_neg)\n",
    "reviews_test_pos_clean1 = preprocess_reviews(reviews_test_pos)\n",
    "reviews_test_neg_clean1 = preprocess_reviews(reviews_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The same postive review example after being cleaned\n",
    "reviews_train_pos_clean1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'story of a man who has unnatural feelings for a pig starts out with a opening scene that is a terrific example of absurd comedy a formal orchestra audience is turned into an insane violent mob by the crazy chantings of its singers unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting even those from the era should be turned off the cryptic dialogue would make shakespeare seem easy to a third grader on a technical level its better than you might think with some good cinematography by future great vilmos zsigmond future stars sally kirkland and frederic forrest can be seen briefly'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The same negative review example after cleaning\n",
    "reviews_train_neg_clean1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Further Text Processing: Removing Stop Words and Normalization\n",
    "\n",
    "Other methods of cleaning the data that can change the model performance include removing stop_words or Normalization (Stemming or Lematization)\n",
    "\n",
    "Stop words are very common words such as 'in', 'of', 'a', 'at', or 'the' that usually don't provide any useful information to the text classifier\n",
    "\n",
    "Normalization (Stemming or Lematization) is a common next step in text preprocessing that converts all the different forms of a certain word into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\patri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "#Stemming\n",
    "def stemming_text(text_data):\n",
    "    stem = PorterStemmer()\n",
    "    return [' '.join([stem.stem(word) for word in review.split()]) for review in text_data]\n",
    "\n",
    "\n",
    "#Lemmatization\n",
    "def lemmatize_text(text_data):\n",
    "    lem = WordNetLemmatizer()\n",
    "    return [' '.join([lem.lemmatize(word) for word in review.split()]) for review in text_data]\n",
    "\n",
    "    \n",
    "#Alternatively there is an easier way to remove stop words by using the stop_words argument with any of scikit-learn’s ‘Vectorizer’ classes\n",
    "#Removing stop words often (but not always) improves the model accuracy   \n",
    "#Need to create the list of stop_words (usually more effective than general lists)\n",
    "#stop_words=['in','of','at','a','the']\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_pos_clean = stemming_text(reviews_train_pos_clean1)\n",
    "reviews_train_neg_clean = stemming_text(reviews_train_neg_clean1)\n",
    "reviews_test_pos_clean = stemming_text(reviews_test_pos_clean1)\n",
    "reviews_test_neg_clean = stemming_text(reviews_test_neg_clean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwel high is a cartoon comedi it ran at the same time as some other program about school life such as teacher my 35 year in the teach profess lead me to believ that bromwel high satir is much closer to realiti than is teacher the scrambl to surviv financi the insight student who can see right through their pathet teacher pomp the petti of the whole situat all remind me of the school i knew and their student when i saw the episod in which a student repeatedli tri to burn down the school i immedi recal at high a classic line inspector im here to sack one of your teacher student welcom to bromwel high i expect that mani adult of my age think that bromwel high is far fetch what a piti that it isnt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_pos_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stori of a man who ha unnatur feel for a pig start out with a open scene that is a terrif exampl of absurd comedi a formal orchestra audienc is turn into an insan violent mob by the crazi chant of it singer unfortun it stay absurd the whole time with no gener narr eventu make it just too off put even those from the era should be turn off the cryptic dialogu would make shakespear seem easi to a third grader on a technic level it better than you might think with some good cinematographi by futur great vilmo zsigmond futur star salli kirkland and freder forrest can be seen briefli'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_neg_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data Sets for Feature Vectorization and Classification Models\n",
    "\n",
    "With these datasets the positve and negative results will be combined and the data needs to be shuffled since the positive and negative data is grouped together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "#Combine all the positive and negative reviews to end up with one training set and one testing set of 25,000 samples each\n",
    "reviews_train = []\n",
    "reviews_train_combined = reviews_train_pos_clean + reviews_train_neg_clean\n",
    "print(len(reviews_train_combined))\n",
    "\n",
    "reviews_test = []\n",
    "reviews_test_combined = reviews_test_pos_clean + reviews_test_neg_clean\n",
    "print(len(reviews_test_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i notic the dvd version seem to have miss scene or line between the post of the frf and the launch they are to prove they can win the right to sit in the frf other than the green team anoth scene is like dure their failur at the simul kevin get joaquin to clam down i think the vh edit other than the abc one might have all the miss stuff otherwis i like to know which dvd releas ha the miss stuff the dvd i have watch feel edit for televis\n",
      "1\n",
      "i own the miniseri on dvd becaus i love thi stori so much thi is one of the best period piec on the civil war that i have seen that tell a stori of friendship divid by the war the costum are great the stori line are great i love how the stori jump around from charact to charact to keep you guess as to what go to happen next in their live there is a great balanc of good and evil some of the charact that are evil in my opinion are too good not to watch everi time someth more despic than the last happen i curs the tv as if they can hear me i love how thi miniseri make me feel engag in it drama i would advis howev not to watch the third movi in the dvd seri it a let down compar to the first two movi and most of the origin charact do not return i dont wish to rememb how the third movi end i prefer to live with the thought of the end in the second movi it make me happi\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Shuffle\n",
    "import random\n",
    "\n",
    "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "c = list(zip(reviews_train_combined, target))\n",
    "random.shuffle(c)\n",
    "reviews_train, y_train = zip(*c)\n",
    "\n",
    "print(reviews_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "b = list(zip(reviews_test_combined, target))\n",
    "random.shuffle(b)\n",
    "reviews_test, y_test = zip(*b)\n",
    "\n",
    "print(reviews_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vectorization (Count Vectorizer and TF-IDF)\n",
    "\n",
    "### Can use either the Unigram Vectorization Features, Bigram Vectorization Features, or TF-IDF Features in the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 65099)\n",
      "  (0, 40455)\t1\n",
      "  (0, 17628)\t3\n",
      "  (0, 61334)\t1\n",
      "  (0, 50640)\t1\n",
      "  (0, 37618)\t3\n",
      "  (0, 49981)\t2\n",
      "  (0, 33693)\t1\n",
      "  (0, 44816)\t1\n",
      "  (0, 22195)\t2\n",
      "  (0, 32813)\t1\n",
      "  (0, 45634)\t1\n",
      "  (0, 63281)\t1\n",
      "  (0, 48125)\t1\n",
      "  (0, 52298)\t1\n",
      "  (0, 24500)\t1\n",
      "  (0, 56541)\t1\n",
      "  (0, 3582)\t1\n",
      "  (0, 33566)\t2\n",
      "  (0, 17577)\t1\n",
      "  (0, 19971)\t1\n",
      "  (0, 52169)\t1\n",
      "  (0, 31322)\t1\n",
      "  (0, 23303)\t1\n",
      "  (0, 30326)\t1\n",
      "  (0, 11562)\t1\n",
      "  :\t:\n",
      "  (24999, 45470)\t1\n",
      "  (24999, 39568)\t1\n",
      "  (24999, 51244)\t1\n",
      "  (24999, 48496)\t1\n",
      "  (24999, 56993)\t1\n",
      "  (24999, 28524)\t1\n",
      "  (24999, 12176)\t1\n",
      "  (24999, 51545)\t1\n",
      "  (24999, 46963)\t1\n",
      "  (24999, 2453)\t1\n",
      "  (24999, 807)\t1\n",
      "  (24999, 48957)\t1\n",
      "  (24999, 51774)\t1\n",
      "  (24999, 34515)\t1\n",
      "  (24999, 48556)\t1\n",
      "  (24999, 29120)\t1\n",
      "  (24999, 16864)\t1\n",
      "  (24999, 12856)\t1\n",
      "  (24999, 49053)\t1\n",
      "  (24999, 40955)\t1\n",
      "  (24999, 58698)\t1\n",
      "  (24999, 3839)\t1\n",
      "  (24999, 23313)\t1\n",
      "  (24999, 9687)\t1\n",
      "  (24999, 29907)\t1\n"
     ]
    }
   ],
   "source": [
    "#Feature Vectorization (Unigram)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#stop_words parameter: If ‘english’, a built-in stop word list for English is used. There are several known issues with ‘english’ and you should consider an alternative (see Using stop words).\n",
    "#ngram_range parameter: The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
    "#max_features parameter: Max number of features\n",
    "vectorizer = CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)\n",
    "\n",
    "X_train_counts = vectorizer.fit_transform(reviews_train)\n",
    "X_test_counts = vectorizer.transform(reviews_test)\n",
    "\n",
    "print(X_train_counts.shape)\n",
    "print(X_train_counts)\n",
    "#print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1619276)\n",
      "  (0, 1201340)\t1\n",
      "  (0, 1107073)\t6\n",
      "  (0, 1471430)\t1\n",
      "  (0, 1525834)\t6\n",
      "  (0, 1033523)\t5\n",
      "  (0, 512986)\t1\n",
      "  (0, 525931)\t7\n",
      "  (0, 585237)\t2\n",
      "  (0, 1120946)\t1\n",
      "  (0, 31913)\t1\n",
      "  (0, 714395)\t1\n",
      "  (0, 766783)\t4\n",
      "  (0, 1533654)\t3\n",
      "  (0, 461134)\t2\n",
      "  (0, 264062)\t5\n",
      "  (0, 189711)\t1\n",
      "  (0, 163689)\t1\n",
      "  (0, 411346)\t1\n",
      "  (0, 280198)\t1\n",
      "  (0, 57346)\t1\n",
      "  (0, 1248573)\t1\n",
      "  (0, 466842)\t2\n",
      "  (0, 1615544)\t2\n",
      "  (0, 722725)\t1\n",
      "  (0, 85274)\t1\n",
      "  :\t:\n",
      "  (24999, 145577)\t1\n",
      "  (24999, 225466)\t1\n",
      "  (24999, 555418)\t1\n",
      "  (24999, 1558913)\t1\n",
      "  (24999, 1500001)\t1\n",
      "  (24999, 273640)\t1\n",
      "  (24999, 494720)\t1\n",
      "  (24999, 676684)\t1\n",
      "  (24999, 190871)\t1\n",
      "  (24999, 652783)\t1\n",
      "  (24999, 1497222)\t1\n",
      "  (24999, 1589267)\t1\n",
      "  (24999, 836086)\t1\n",
      "  (24999, 303095)\t1\n",
      "  (24999, 293695)\t1\n",
      "  (24999, 1291263)\t1\n",
      "  (24999, 433972)\t1\n",
      "  (24999, 269836)\t1\n",
      "  (24999, 591330)\t1\n",
      "  (24999, 767600)\t1\n",
      "  (24999, 1416490)\t1\n",
      "  (24999, 1176027)\t1\n",
      "  (24999, 1433558)\t1\n",
      "  (24999, 625455)\t1\n",
      "  (24999, 778992)\t1\n"
     ]
    }
   ],
   "source": [
    "#Feature Vectorization (Bigram)\n",
    "vectorizer_bigram = CountVectorizer(stop_words = stop_words, ngram_range=(1,2), max_features = None)\n",
    "\n",
    "X_train_counts_bigram = vectorizer_bigram.fit_transform(reviews_train)\n",
    "X_test_counts_bigram = vectorizer_bigram.transform(reviews_test)\n",
    "\n",
    "print(X_train_counts_bigram.shape)\n",
    "print(X_train_counts_bigram)\n",
    "#print(vectorizer_bigram.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 65099)\n",
      "  (0, 63281)\t0.11083433945185653\n",
      "  (0, 62327)\t0.0507694016731053\n",
      "  (0, 61393)\t0.1412441845719497\n",
      "  (0, 61334)\t0.09573953706858233\n",
      "  (0, 57345)\t0.05973494906804011\n",
      "  (0, 56681)\t0.11412279719408029\n",
      "  (0, 56541)\t0.11664127115151429\n",
      "  (0, 54993)\t0.21188412569678558\n",
      "  (0, 52298)\t0.10301376233614919\n",
      "  (0, 52169)\t0.18219861396704948\n",
      "  (0, 50640)\t0.06516592696918881\n",
      "  (0, 49981)\t0.11847839563543708\n",
      "  (0, 48125)\t0.07956306288925058\n",
      "  (0, 47420)\t0.09535043606171328\n",
      "  (0, 45634)\t0.11074553693310377\n",
      "  (0, 44816)\t0.12267723647819541\n",
      "  (0, 41841)\t0.11806021793607682\n",
      "  (0, 41337)\t0.03940341655485488\n",
      "  (0, 40455)\t0.11456558641740014\n",
      "  (0, 37618)\t0.27308218183970406\n",
      "  (0, 37199)\t0.08442622861602904\n",
      "  (0, 33693)\t0.08064682178545544\n",
      "  (0, 33566)\t0.0855808984205681\n",
      "  (0, 32813)\t0.1646457066677816\n",
      "  (0, 31823)\t0.0633881927598869\n",
      "  :\t:\n",
      "  (24999, 27080)\t0.09436819606011101\n",
      "  (24999, 25475)\t0.12292227697661863\n",
      "  (24999, 23792)\t0.05759397248227639\n",
      "  (24999, 23313)\t0.2420442543493115\n",
      "  (24999, 21699)\t0.09797692689344248\n",
      "  (24999, 20853)\t0.03773395167992363\n",
      "  (24999, 19556)\t0.12365319961964387\n",
      "  (24999, 18698)\t0.07346038605684521\n",
      "  (24999, 16864)\t0.1861006025631659\n",
      "  (24999, 16584)\t0.0727466867673293\n",
      "  (24999, 16569)\t0.06718254540820555\n",
      "  (24999, 15814)\t0.0739144018291805\n",
      "  (24999, 14658)\t0.15190001555324123\n",
      "  (24999, 12856)\t0.15441568244066461\n",
      "  (24999, 12176)\t0.113772601393669\n",
      "  (24999, 12120)\t0.1212318540762007\n",
      "  (24999, 9687)\t0.23196861740456917\n",
      "  (24999, 6872)\t0.07918177316790377\n",
      "  (24999, 6704)\t0.06684090916373132\n",
      "  (24999, 6653)\t0.06535659915403462\n",
      "  (24999, 6146)\t0.08072424684263532\n",
      "  (24999, 5795)\t0.0952646708495796\n",
      "  (24999, 3839)\t0.19281720327103136\n",
      "  (24999, 2453)\t0.1123542168328896\n",
      "  (24999, 807)\t0.11635914047263822\n"
     ]
    }
   ],
   "source": [
    "#Feature Vectorization (TF-IDF)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Classifier Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = reviews_train\n",
    "X_test = reviews_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train_counts, target, train_size=0.8, test_size = 0.2)\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_val.shape)\n",
    "#print(X_val.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Pipeline for Cross Validation\n",
    "\n",
    "A pipeline for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.88288 0.88284 0.88284 0.88284 0.89724 0.8976  0.89768 0.89836 0.88288\n",
      " 0.88284 0.88284 0.88284 0.89724 0.8976  0.89768 0.89836]\n",
      "scores_std [0.00426633 0.00422261 0.00422261 0.00422261 0.00539985 0.00447392\n",
      " 0.00503206 0.00470982 0.00426633 0.00422261 0.00422261 0.00422261\n",
      " 0.00539985 0.00447392 0.00503206 0.00470982]\n",
      "Best score: 0.898\n",
      "\n",
      " Best Parameter Values: \n",
      "clf__max_iter: 5000\n",
      "clf__penalty: 'none'\n",
      "clf__tol: 0.0001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "#Tol parameter:Tolerance for stopping criteria\n",
    "#penalty parameter:Used to specify the norm used in the penalization\n",
    "#max_iter parameter: Maximum number of iterations\n",
    "parameters = {'clf__tol': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "             'clf__penalty': ['l2','none'],\n",
    "             'clf__max_iter': [5000 , 10000]}\n",
    "n_folds = 5\n",
    "\n",
    "LR_GridSearch = GridSearchCV(pipeline, param_grid = parameters, cv=n_folds)\n",
    "LR_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = LR_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = LR_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % LR_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, LR_GridSearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [1 1 0 ... 0 0 0]\n",
      "Accuracy:  88.612 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pipeline = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(penalty = 'none', max_iter = 5000, tol = 0.0001))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_predLR = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Predicted: \", y_predLR)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_predLR)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.8866  0.88664 0.88664 0.88664 0.8648  0.86424 0.8644  0.86448 0.8866\n",
      " 0.88664 0.88664 0.88664 0.8648  0.86424 0.8644  0.86448]\n",
      "scores_std [0.00542144 0.00540133 0.00540133 0.00540133 0.00309063 0.00169894\n",
      " 0.00165892 0.00133026 0.00542144 0.00540133 0.00540133 0.00540133\n",
      " 0.00309063 0.00169894 0.00165892 0.00133026]\n",
      "Best score: 0.887\n",
      "\n",
      " Best Parameter Values: \n",
      "clf__max_iter: 5000\n",
      "clf__penalty: 'l2'\n",
      "clf__tol: 0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "#Tol parameter:Tolerance for stopping criteria\n",
    "#penalty parameter:Used to specify the norm used in the penalization\n",
    "#max_iter parameter: Maximum number of iterations\n",
    "parameters = {'clf__tol': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "             'clf__penalty': ['l2','none'],\n",
    "             'clf__max_iter': [5000 , 10000]}\n",
    "n_folds = 5\n",
    "\n",
    "LR_GridSearch = GridSearchCV(pipeline, param_grid = parameters, cv=n_folds)\n",
    "LR_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = LR_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = LR_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % LR_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, LR_GridSearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [1 1 1 ... 0 0 0]\n",
      "Accuracy:  87.936 %\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(penalty = 'l2', max_iter = 5000, tol = 0.01))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_predLR = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Predicted: \", y_predLR)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_predLR)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Which Features (Words) Were the Most Valuable for Classification as Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most likely indicating a positive review: \n",
      "('refresh', 1.775541146209219)\n",
      "('squirrel', 1.6164542665162633)\n",
      "('flawless', 1.6063634723467413)\n",
      "('excel', 1.3909007943743532)\n",
      "('driven', 1.3722638977786057)\n",
      "('superb', 1.3652533702337164)\n",
      "('erot', 1.365220575726894)\n",
      "('perfectli', 1.3089998175988216)\n",
      "('carrey', 1.3056679831978968)\n",
      "('whoopi', 1.2832965955827118)\n",
      "('milligan', 1.2661321349155572)\n",
      "('kitti', 1.264983806912379)\n",
      "('funniest', 1.2582876116888642)\n",
      "('kurosawa', 1.2576825609486126)\n",
      "('mj', 1.2458624418083457)\n",
      "('superbl', 1.2335502183379232)\n",
      "('joli', 1.2258965212814503)\n",
      "('surprisingli', 1.2257760633147008)\n",
      "('favorit', 1.2057191725323264)\n",
      "('vengeanc', 1.1922249867344088)\n",
      "\n",
      " Words most likely indicating a negative review: \n",
      "('worst', -1.9812163757246914)\n",
      "('poorli', -1.9246558229386028)\n",
      "('aw', -1.7424176007493648)\n",
      "('wast', -1.6024972276780391)\n",
      "('unfunni', -1.5475552739510916)\n",
      "('baldwin', -1.5360089985870147)\n",
      "('unwatch', -1.510896658641225)\n",
      "('forgett', -1.4828496296362064)\n",
      "('alright', -1.472510050937825)\n",
      "('badli', -1.451969860047038)\n",
      "('mst3k', -1.4465463666278942)\n",
      "('britney', -1.433134408172644)\n",
      "('assumpt', -1.3633002158891794)\n",
      "('insult', -1.3486028992794121)\n",
      "('mess', -1.3357698565995477)\n",
      "('uninterest', -1.3319033274972876)\n",
      "('mildli', -1.3210129649923952)\n",
      "('hype', -1.3120592191592246)\n",
      "('bore', -1.3093692808265005)\n",
      "('lousi', -1.302746578905209)\n"
     ]
    }
   ],
   "source": [
    "clf_LR = LogisticRegression(penalty = 'l2', max_iter = 5000, tol = 0.01)\n",
    "clf_LR.fit(X_train_counts, y_train)\n",
    "pred = clf_LR.predict(X_test_counts)\n",
    "\n",
    "feature_to_coef = {word: coef for word, coef in zip(vectorizer.get_feature_names(), clf_LR.coef_[0])}\n",
    "\n",
    "print(\"Words most likely indicating a positive review: \")\n",
    "for most_positive in sorted(feature_to_coef.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print (most_positive)\n",
    "  \n",
    "print(\"\\n Words most likely indicating a negative review: \")\n",
    "for most_negative in sorted(feature_to_coef.items(), key=lambda x: x[1])[:20]:\n",
    "    print (most_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.86052 0.861   0.8596  0.85672]\n",
      "scores_std [0.00592972 0.00563276 0.00456246 0.00506691]\n",
      "Best score: 0.861\n",
      "\n",
      " Best Parameter Values: \n",
      "clf__alpha: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipelineMNB = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "#alpha parameter: Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing)\n",
    "parameters = {'clf__alpha': [1, 0.5, 0.2, 0.1]}\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "MNB_GridSearch = GridSearchCV(pipelineMNB, param_grid = parameters, cv=n_folds)\n",
    "MNB_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = MNB_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = MNB_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % MNB_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, MNB_GridSearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [0 0 1 ... 1 1 0]\n",
      "Accuracy:  81.64 %\n"
     ]
    }
   ],
   "source": [
    "pipelineMNB = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB(alpha = 0.5))\n",
    "])\n",
    "\n",
    "pipelineMNB.fit(X_train, y_train)\n",
    "y_predMNB = pipelineMNB.predict(X_test)\n",
    "\n",
    "print(\"Predicted: \", y_predMNB)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_predMNB)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.5282  0.67976 0.67828 0.631   0.63536 0.70692 0.70148 0.73924 0.76764\n",
      " 0.80968 0.80496 0.80788 0.85416 0.85432 0.85472 0.84432 0.84396 0.84504\n",
      " 0.88856 0.88824 0.88796 0.88064 0.88084 0.88108]\n",
      "scores_std [0.03611869 0.09731166 0.10389176 0.09053424 0.13215981 0.10290085\n",
      " 0.10765891 0.08041447 0.03955784 0.00785529 0.00484545 0.00804249\n",
      " 0.00383124 0.00442647 0.00437557 0.0045477  0.00448357 0.00472508\n",
      " 0.00599386 0.00583287 0.00526027 0.00539392 0.00502538 0.00529392]\n",
      "Best score: 0.889\n",
      "\n",
      " Best Parameter Values: \n",
      "clf__alpha: 0.0001\n",
      "clf__loss: 'hinge'\n",
      "clf__max_iter: 60\n",
      "clf__penalty: 'l2'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "pipelineSGD = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier())\n",
    "])\n",
    "\n",
    "#loss parameter:The loss function to be used\n",
    "#penalty parameter:The penalty (aka regularization term) to be used\n",
    "#alpha parameter:Constant that multiplies the regularization term\n",
    "#max_iter parameter:The maximum number of passes over the training data (aka epochs)\n",
    "parameters = {'clf__loss': ['hinge', 'log'],\n",
    "             'clf__penalty': ['l2'],\n",
    "             'clf__alpha': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "             'clf__max_iter': [60, 80, 100]}\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "SGD_GridSearch = GridSearchCV(pipelineSGD, param_grid = parameters, cv=n_folds)\n",
    "SGD_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = SGD_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = SGD_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % SGD_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, SGD_GridSearch.best_params_[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [0 0 1 ... 1 0 0]\n",
      "Accuracy:  88.016 %\n"
     ]
    }
   ],
   "source": [
    "pipelineSGD = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, max_iter=60))\n",
    "])\n",
    "\n",
    "pipelineSGD.fit(X_train, y_train)\n",
    "\n",
    "y_predSGD = pipelineSGD.predict(X_test)\n",
    "print(\"Predicted: \", y_predSGD)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_predSGD)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.63032 0.6052  0.71284 0.558   0.51032 0.69992 0.59132 0.509   0.7312\n",
      " 0.62884 0.526   0.73364 0.6438  0.55252 0.7252  0.63452 0.60044 0.71128\n",
      " 0.54944 0.50976 0.6946  0.58444 0.51756 0.72736 0.6116  0.52328 0.72448\n",
      " 0.64912 0.54852 0.71656]\n",
      "scores_std [0.01242617 0.01341223 0.00355843 0.04123358 0.00694158 0.00675941\n",
      " 0.01966381 0.00609393 0.00554977 0.0126639  0.01429014 0.00379768\n",
      " 0.00920608 0.01870213 0.00517378 0.00884498 0.00630098 0.00541051\n",
      " 0.00877396 0.00734019 0.00489571 0.01568816 0.01024687 0.00866016\n",
      " 0.01298984 0.00853004 0.00710785 0.0148338  0.01739775 0.0048405 ]\n",
      "Best score: 0.734\n",
      "\n",
      " Best Parameter Values: \n",
      "clf__criterion: 'gini'\n",
      "clf__max_depth: 20\n",
      "clf__max_features: None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipeline_tree = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "#criterion parameter:The function to measure the quality of a split.\n",
    "#max_depth parameter:The maximum depth of the tree. \n",
    "#max_features parameter:The number of features to consider when looking for the best split. If “sqrt”, then max_features=sqrt(n_features). If “log2”, then max_features=log2(n_features).\n",
    "parameters = {'clf__criterion': ['gini', 'entropy'],\n",
    "             'clf__max_depth': [None, 5, 10, 20, 40],\n",
    "             'clf__max_features': ['sqrt', 'log2', None]}\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "tree_GridSearch = GridSearchCV(pipeline_tree, param_grid = parameters, cv=n_folds)\n",
    "tree_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = tree_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = tree_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % tree_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, tree_GridSearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [1 1 0 ... 0 1 1]\n",
      "Accuracy:  73.584 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "pipeline_tree = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', DecisionTreeClassifier(criterion ='gini', max_depth = 20, max_features = None))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = pipeline_tree.predict(X_test)\n",
    "print(\"Predicted: \", y_pred_tree)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred_tree)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.88512 0.88532 0.88532 0.88532 0.88532 0.8854  0.88532 0.88532 0.88524\n",
      " 0.88532 0.88532 0.88532 0.89016 0.89032 0.89032 0.89032 0.89012 0.89032\n",
      " 0.89032 0.89032 0.89024 0.89036 0.89032 0.89032]\n",
      "scores_std [0.00540903 0.00584137 0.00584137 0.00584137 0.00553332 0.00578273\n",
      " 0.00584137 0.00584137 0.00588068 0.00584137 0.00584137 0.00584137\n",
      " 0.00534625 0.00534954 0.00540607 0.00540607 0.00555496 0.00540607\n",
      " 0.00540607 0.00540607 0.00534775 0.00536269 0.00540607 0.00540607]\n",
      "Best score: 0.890\n",
      "\n",
      " Best Parameter Values: \n",
      "clf__C: 0.5\n",
      "clf__max_iter: 2000\n",
      "clf__penalty: 'l2'\n",
      "clf__tol: 0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pipelineSVM = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "#penalty parameter:Specifies the norm used in the penalization\n",
    "#tol parameter:Tolerance for stopping criteria.\n",
    "#C parameter: Regularization parameter. The strength of the regularization is inversely proportional to C.\n",
    "#max_iter parameter:The maximum number of iterations to be run\n",
    "parameters = {'clf__penalty': ['l2'],\n",
    "             'clf__tol': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'clf__C': [1.0, 0.5],\n",
    "             'clf__max_iter': [1000, 1500, 2000]}\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "SVM_GridSearch = GridSearchCV(pipelineSVM, param_grid = parameters, cv=n_folds)\n",
    "SVM_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = SVM_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = SVM_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % SVM_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, SVM_GridSearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [1 1 0 ... 0 0 0]\n",
      "Accuracy:  87.348 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "pipelineSVM = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC(C = 0.5, max_iter = 2000, penalty = 'l2', tol = 0.01))\n",
    "])\n",
    "\n",
    "\n",
    "pipelineSVM.fit(X_train, y_train)\n",
    "\n",
    "y_predSVM = pipelineSVM.predict(X_test)\n",
    "print(\"Predicted: \", y_predSVM)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_predSVM)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.7374  0.7708  0.78808 0.79952 0.80008 0.82876 0.8374  0.8464  0.80456\n",
      " 0.82744 0.83504 0.8392 ]\n",
      "scores_std [0.00848622 0.00486785 0.0056831  0.00597006 0.00572797 0.00587115\n",
      " 0.00448464 0.00631189 0.00494028 0.00351545 0.00232    0.00364198]\n",
      "Best score: 0.846\n",
      "\n",
      " Best Parameter Values: \n",
      "clf__base_estimator: None\n",
      "clf__learning_rate: 0.5\n",
      "clf__n_estimators: 200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "pipelineADA = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "#base_estimator parameter: The base estimator from which the boosted ensemble is built.\n",
    "#n_estimators: The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n",
    "#learning_rate:Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators.\n",
    "parameters = {'clf__base_estimator': [None],\n",
    "              'clf__n_estimators': [50, 100, 150, 200],\n",
    "             'clf__learning_rate': [0.1, 0.5, 1]}\n",
    "n_folds = 5\n",
    "\n",
    "Ada_GridSearch = GridSearchCV(pipelineADA, param_grid = parameters, cv=n_folds)\n",
    "Ada_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = Ada_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = Ada_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % Ada_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, Ada_GridSearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [1 1 0 ... 0 0 0]\n",
      "Accuracy:  86.116 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "pipelineADA = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', AdaBoostClassifier(base_estimator = None, n_estimators = 500, learning_rate = 0.5))\n",
    "])\n",
    "\n",
    "pipelineADA.fit(X_train, y_train)\n",
    "\n",
    "y_pred_Ada = pipelineADA.predict(X_test)\n",
    "print(\"Predicted: \", y_pred_Ada)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred_Ada)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipelineForest = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "#n_estimators parameter:The number of trees in the forest\n",
    "#criterion parameter:The function to measure the quality of a split (gini or entropy)\n",
    "#max_depth parameter:The maximum depth of the tree.\n",
    "#max_features parameter:The number of features to consider when looking for the best split. If “sqrt”, then max_features=sqrt(n_features) (same as “auto”). If “log2”, then max_features=log2(n_features). If None, then max_features=n_features.\n",
    "#bootstrap parameter: Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree.\n",
    "parameters = {'clf__n_estimators': [50, 70, 90],\n",
    "             'clf__criterion': ['gini', 'entropy'],\n",
    "             'clf__max_depth': [None, 10, 20],\n",
    "             'clf__max_features': ['sqrt', 'log2', None],\n",
    "             'clf__bootstrap': [True, False]}\n",
    "n_folds = 5\n",
    "\n",
    "forest_GridSearch = GridSearchCV(pipelineForest, param_grid = parameters, cv=n_folds)\n",
    "forest_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "scores = forest_GridSearch.cv_results_['mean_test_score']\n",
    "scores_std = forest_GridSearch.cv_results_['std_test_score']\n",
    "\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "print(\"Best score: %0.3f\" % forest_GridSearch.best_score_)\n",
    "\n",
    "print(\"\\n Best Parameter Values: \")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, forest_GridSearch.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [1 1 1 ... 0 1 0]\n",
      "Accuracy:  85.124 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipelineForest = Pipeline([ \n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,1), max_features = None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier(n_estimators = 120, criterion = 'entropy', max_depth = None, max_features = 'sqrt', bootstrap = False))\n",
    "])\n",
    "\n",
    "pipelineForest.fit(X_train, y_train)\n",
    "\n",
    "y_pred_forest = pipelineForest.predict(X_test)\n",
    "print(\"Predicted: \", y_pred_forest)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred_forest)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze which Model is the Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:  87.936 %\n",
      "Logistic Regression Recall:  87.944 %\n",
      "Logistic Regression Precision:  87.92993121100623 % \n",
      "\n",
      "Decision Tree Accuracy:  73.584 %\n",
      "Decision Tree Recall:  80.71199999999999 %\n",
      "Decision Tree Precision:  70.64136675535639 % \n",
      "\n",
      "SVM Accuracy:  87.348 %\n",
      "SVM Recall:  86.344 %\n",
      "SVM Precision:  88.11331537268349 % \n",
      "\n",
      "Adaboost Accuracy:  86.116 %\n",
      "Adaboost Recall:  87.8 %\n",
      "Adaboost Precision:  84.93924618837552 % \n",
      "\n",
      "Random Forest Accuracy:  85.124 %\n",
      "Random Forest Recall:  84.91199999999999 %\n",
      "Random Forest Precision:  85.27355989395035 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Accuracy: \", metrics.accuracy_score(y_test, y_predLR)*100, \"%\")\n",
    "print(\"Logistic Regression Recall: \", metrics.recall_score(y_test, y_predLR)*100, \"%\")\n",
    "print(\"Logistic Regression Precision: \", metrics.precision_score(y_test, y_predLR)*100, \"%\", '\\n')\n",
    "\n",
    "print(\"Decision Tree Accuracy: \", metrics.accuracy_score(y_test, y_pred_tree)*100, \"%\")\n",
    "print(\"Decision Tree Recall: \", metrics.recall_score(y_test, y_pred_tree)*100, \"%\")\n",
    "print(\"Decision Tree Precision: \", metrics.precision_score(y_test, y_pred_tree)*100, \"%\", '\\n')\n",
    "\n",
    "print(\"SVM Accuracy: \", metrics.accuracy_score(y_test, y_predSVM)*100, \"%\")\n",
    "print(\"SVM Recall: \", metrics.recall_score(y_test, y_predSVM)*100, \"%\")\n",
    "print(\"SVM Precision: \", metrics.precision_score(y_test, y_predSVM)*100, \"%\", '\\n')\n",
    "\n",
    "print(\"Adaboost Accuracy: \", metrics.accuracy_score(y_test, y_pred_Ada)*100, \"%\")\n",
    "print(\"Adaboost Recall: \", metrics.recall_score(y_test, y_pred_Ada)*100, \"%\")\n",
    "print(\"Adaboost Precision: \", metrics.precision_score(y_test, y_pred_Ada)*100, \"%\", '\\n')\n",
    "\n",
    "print(\"Random Forest Accuracy: \", metrics.accuracy_score(y_test, y_pred_forest)*100, \"%\")\n",
    "print(\"Random Forest Recall: \", metrics.recall_score(y_test, y_pred_forest)*100, \"%\")\n",
    "print(\"Random Forest Precision: \", metrics.precision_score(y_test, y_pred_forest)*100, \"%\", '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
